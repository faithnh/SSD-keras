{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FoLzWPR-dI2v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "!rm -rf SSD-keras# Untitled Notebook\n",
        "\n",
        "This is an initial placeholder notebook. Feel free to edit and rename as well as create your own notebooks, to use Google Cloud Datalab."
      ]
    },
    {
      "metadata": {
        "id": "-v6kIQ94dI2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "!rm -rf SSD-keras\n",
        "\n",
        "!git clone https://github.com/faithnh/SSD-keras.git -b run-tensorflow\n",
        "\n",
        "os.chdir('/content/SSD-keras')\n",
        "\n",
        "import sys\n",
        "sys.path.append('src')\n",
        "\n",
        "!ls\n",
        "\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "56t5HhVrhWVr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!chmod +x ./datasets/VOC0712_downloader.sh\n",
        "\n",
        "!./datasets/VOC0712_downloader.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F7bpWNmJfY6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from models import SSD300\n",
        "from utils.boxes import create_prior_boxes, to_point_form\n",
        "from utils.data_management import DataManager, get_class_names\n",
        "from utils.data_generator import DataGenerator\n",
        "from utils.training import MultiboxLoss, scheduler\n",
        "\n",
        "\n",
        "model_name = 'SSD300_VOC2007'\n",
        "# hyper-parameters\n",
        "batch_size = 2\n",
        "num_epochs = 250\n",
        "alpha_loss = 1.0\n",
        "learning_rate = 1e-3\n",
        "momentum = .9\n",
        "weight_decay = 5e-4\n",
        "gamma_decay = 0.1\n",
        "# scheduled_epochs = [155, 195, 235]\n",
        "negative_positive_ratio = 3\n",
        "base_weights_path = '../trained_models/VGG16_weights.hdf5'\n",
        "\n",
        "# data\n",
        "class_names = get_class_names('VOC2007')\n",
        "val_dataset, val_split = 'VOC2007', 'test'\n",
        "train_datasets, train_splits = ['VOC2007', 'VOC2012'], ['trainval', 'trainval']\n",
        "train_data_manager = DataManager(train_datasets, train_splits, class_names)\n",
        "train_data = train_data_manager.load_data()\n",
        "num_classes = len(class_names)\n",
        "val_data_manager = DataManager(val_dataset, val_split, class_names)\n",
        "val_data = val_data_manager.load_data()\n",
        "\n",
        "# model\n",
        "model = SSD300(num_classes=num_classes)\n",
        "model.load_weights(base_weights_path, by_name=True)\n",
        "prior_boxes = to_point_form(create_prior_boxes())\n",
        "multibox_loss = MultiboxLoss(num_classes, negative_positive_ratio, alpha_loss)\n",
        "optimizer = SGD(learning_rate, momentum, weight_decay)\n",
        "model.compile(optimizer, loss=multibox_loss.compute_loss)\n",
        "data_generator = DataGenerator(\n",
        "        train_data, prior_boxes, batch_size, num_classes, val_data)\n",
        "\n",
        "# callbacks\n",
        "model_path = '../trained_models/' + model_name + '/'\n",
        "save_path = model_path + 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "log = CSVLogger(model_path + model_name + '.log')\n",
        "checkpoint = ModelCheckpoint(save_path, verbose=1, save_weights_only=True)\n",
        "# reduce_on_plateau = ReduceLROnPlateau(factor=gamma_decay, verbose=1)\n",
        "# scheduler = LearningRateManager(learning_rate, gamma_decay, scheduled_epochs)\n",
        "learning_rate_schedule = LearningRateScheduler(scheduler, verbose=1)\n",
        "callbacks = [checkpoint, log, learning_rate_schedule]\n",
        "# callbacks = [checkpoint, log, reduce_on_plateau]\n",
        "\n",
        "# training\n",
        "model.summary()\n",
        "model.fit_generator(data_generator.flow(mode='train'),\n",
        "                    steps_per_epoch=int(len(train_data) / batch_size),\n",
        "                    epochs=num_epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=data_generator.flow(mode='val'),\n",
        "                    validation_steps=int(len(val_data) / batch_size),\n",
        "                    use_multiprocessing=False,\n",
        "                    workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}